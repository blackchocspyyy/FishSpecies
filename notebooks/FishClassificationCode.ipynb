{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full script from loading the dataset to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 18:45:58.275594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import scipy.stats as stats\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from itertools import product\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Masking\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "from data_utils import load_fish_csvs, merge_and_parse_timestamps\n",
    "from feature_utils import (\n",
    "    select_frequency_columns,\n",
    "    reduce_features_by_variance_and_correlation,\n",
    "    get_top_features_by_random_forest,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "from training_utils import (\n",
    "    evaluate_lofo_models, \n",
    "    evaluate_lofo_xgboost_multi, \n",
    "    tune_xgboost_with_cv_multi, \n",
    "    evaluate_lofo_xgboost_smote, \n",
    "    evaluate_lofo_rf_smote,\n",
    "    evaluate_lstm_lofo_kfold\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reloads the training_utils module to pick up code changes.\n",
    "# import importlib\n",
    "# import training_utils\n",
    "\n",
    "# importlib.reload(training_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data files containing fish \n",
    "df_list = ['LT009.csv', 'LT010.csv', 'LT011.csv', \n",
    "           'LT012.csv', 'LT014.csv', 'LT016.csv', 'LT017.csv', \n",
    "           'LT018.csv', 'LT021.csv', 'SMB001.csv', 'SMB002.csv', \n",
    "           'SMB005.csv', 'SMB006.csv', 'SMB007.csv',\n",
    "           'SMB011.csv', 'SMB012.csv']\n",
    "# Load all CSV files into separate DataFrames\n",
    "dataframes = load_fish_csvs(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge timestamps\n",
    "df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "df = merge_and_parse_timestamps(df) \n",
    "\n",
    "# Frequency selection\n",
    "f_number_cols = select_frequency_columns(df)\n",
    "df_filtered = df[[\"fishNum\", \"species\", \"dateProcessed\", \"Ping_time\"] + f_number_cols].copy()\n",
    "df_filtered['species_label'] = df_filtered['species'].astype('category').cat.codes\n",
    "\n",
    "# Feature reduction\n",
    "selected_features, corr_matrix = reduce_features_by_variance_and_correlation(df_filtered, f_number_cols)\n",
    "\n",
    "# RF selection\n",
    "top_features_rf = get_top_features_by_random_forest(df_filtered, list(selected_features), 'species_label')\n",
    "\n",
    "# Standardize for PCA\n",
    "X = df_filtered[top_features_rf]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "X_pca, pca_variance = apply_pca(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LOFO evaluation...\n",
      "\n",
      "Model Performance Summary:\n",
      "            Mean Accuracy\n",
      "LogReg_RF           0.662\n",
      "LogReg_PCA          0.661\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for LOFO evaluation\n",
    "X_rf_selected = df_filtered[top_features_rf]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    # Logistic Regression with RF-selected features\n",
    "    \"LogReg_RF\": LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000, \n",
    "        random_state=42,\n",
    "        solver='saga'  \n",
    "    ),\n",
    "    \n",
    "    # Logistic Regression with PCA features\n",
    "    \"LogReg_PCA\": LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        penalty='l2',  \n",
    "        C=0.1  # Stronger regularization for PCA features\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "print(\"Running LOFO evaluation...\")\n",
    "results, summary_df = evaluate_lofo_models(df_filtered, X_rf_selected, X_pca, models)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(summary_df.round(3))  # Round to 3 decimal places\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost LOFO evaluation...\n",
      "\n",
      "XGBoost Model Comparison:\n",
      "                     Mean Accuracy\n",
      "XGB_RF_Selected              0.562\n",
      "XGB_PCA_Transformed          0.550\n"
     ]
    }
   ],
   "source": [
    "feature_sets = {\n",
    "    # Random Forest selected features (original feature space)\n",
    "    \"XGB_RF_Selected\": X_rf_selected,\n",
    "    \n",
    "    # PCA-transformed features \n",
    "    \"XGB_PCA_Transformed\": pd.DataFrame(X_pca, index=df_filtered.index)\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "print(\"Starting XGBoost LOFO evaluation...\")\n",
    "results_xgb_multi, summary_xgb_multi = evaluate_lofo_xgboost_multi(\n",
    "    df_filtered, \n",
    "    feature_sets, \n",
    "    groups=df_filtered['fishNum']  \n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nXGBoost Model Comparison:\")\n",
    "print(summary_xgb_multi.round(3)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost hyperparameter tuning...\n",
      "Tuning XGBoost for: XGB_RF_Selected\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Tuning XGBoost for: XGB_PCA_Transformed\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "XGBoost Tuning Results:\n",
      "            Model Name  Best Accuracy\n",
      "1  XGB_PCA_Transformed          0.723\n",
      "0      XGB_RF_Selected          0.721\n"
     ]
    }
   ],
   "source": [
    "feature_sets = {\n",
    "    \"XGB_RF_Selected\": X_rf_selected,\n",
    "    \"XGB_PCA_Transformed\": pd.DataFrame(X_pca, index=df_filtered.index)\n",
    "}\n",
    "\n",
    "print(\"Starting XGBoost hyperparameter tuning...\")\n",
    "summary_df_xgb_tuning = tune_xgboost_with_cv_multi(\n",
    "    df_filtered, \n",
    "    feature_sets, \n",
    "    df_filtered['species_label']  \n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nXGBoost Tuning Results:\")\n",
    "print(summary_df_xgb_tuning[[\"Model Name\", \"Best Accuracy\"]].round(3).sort_values(\"Best Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SMOTE to balance data\n",
    "SMOTE creates synthetic samples to balance class distribution\n",
    "#### XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with SMOTE Results:\n",
      "          Model Name  Mean Accuracy\n",
      "0  XGB_RF_Stat+SMOTE          0.688\n"
     ]
    }
   ],
   "source": [
    "# Evaluate XGBoost with SMOTE balancing\n",
    "summary_df_smote = evaluate_lofo_xgboost_smote(\n",
    "    df_filtered,         \n",
    "    top_features_rf      # Best features from Random Forest selection\n",
    ")\n",
    "print(\"XGBoost with SMOTE Results:\")\n",
    "print(summary_df_smote.round(3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest with SMOTE Results:\n",
      "         Model Name  Mean LOFO Accuracy\n",
      "0  RF_RF_Stat+SMOTE               0.688\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest with SMOTE balancing\n",
    "summary_df_rf_smote = evaluate_lofo_rf_smote(\n",
    "    df_filtered,         \n",
    "    top_features_rf      \n",
    ")\n",
    "print(\"\\nRandom Forest with SMOTE Results:\")\n",
    "print(summary_df_rf_smote.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML models performed not well, they did not incorporate time series nature in this dataset. We need to consider using models designed for time-series.\n",
    "Besides, leave one fish out makes the test set to have only one species. It's better to leave a pair of fish out instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by fish and time to maintain sequence order\n",
    "df_lstm = df_filtered.sort_values(by=[\"fishNum\", \"Ping_time\"])\n",
    "\n",
    "# Select frequency feature columns\n",
    "frequency_cols = df_lstm.columns[4:-1]\n",
    "\n",
    "# Normalize the frequency features\n",
    "scaler = StandardScaler()\n",
    "df_lstm[frequency_cols] = scaler.fit_transform(df_lstm[frequency_cols])\n",
    "\n",
    "# Group by fish to create sequences\n",
    "grouped = df_lstm.groupby(\"fishNum\")\n",
    "\n",
    "fish_sequences = []\n",
    "fish_labels = []\n",
    "fish_nums = []\n",
    "\n",
    "for fish_id, group in grouped:\n",
    "    # Store sequence with shape (timesteps, features)\n",
    "    fish_sequences.append(group[frequency_cols].values)\n",
    "    fish_labels.append(group[\"species\"].iloc[0])\n",
    "    fish_nums.append(fish_id)\n",
    "\n",
    "fish_sequences = np.array(fish_sequences, dtype=object)\n",
    "fish_labels = np.array(fish_labels)\n",
    "\n",
    "# Pad to same length\n",
    "max_timesteps = max([seq.shape[0] for seq in fish_sequences])\n",
    "fish_sequences_padded = pad_sequences(fish_sequences, maxlen=max_timesteps, dtype=\"float32\", padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Encode labels\n",
    "species_mapping = {species: idx for idx, species in enumerate(np.unique(fish_labels))}\n",
    "fish_labels_encoded = np.array([species_mapping[label] for label in fish_labels])\n",
    "\n",
    "# LOPO pair generation\n",
    "lt_fish = [fish for fish, label in zip(fish_nums, fish_labels_encoded) if label == 0]\n",
    "smb_fish = [fish for fish, label in zip(fish_nums, fish_labels_encoded) if label == 1]\n",
    "lopo_pairs = list(product(lt_fish, smb_fish))\n",
    "random.seed(42)\n",
    "random_lopo_pairs = random.sample(lopo_pairs, 5)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (max_timesteps, fish_sequences_padded.shape[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LOPO on 5 pairs...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x14f956320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1504568c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 905ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828ms/step\n",
      "\n",
      "LSTM Performance Summary:\n",
      "                         0\n",
      "Model Name            LSTM\n",
      "Mean LOPO Accuracy     0.8\n",
      "Mean K-Fold Accuracy   0.8\n"
     ]
    }
   ],
   "source": [
    "summary_df_lstm = evaluate_lstm_lofo_kfold(\n",
    "    fish_sequences_padded=fish_sequences_padded,\n",
    "    fish_labels_encoded=fish_labels_encoded,\n",
    "    fish_nums=fish_nums,\n",
    "    lopo_pairs=lopo_pairs,\n",
    "    random_lopo_pairs=random_lopo_pairs,\n",
    "    input_shape=input_shape,\n",
    "    use_all_pairs=False  \n",
    ")\n",
    "print(\"\\nLSTM Performance Summary:\")\n",
    "print(summary_df_lstm.round(3).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LOPO on 63 pairs...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 749ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 763ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 824ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 877ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 875ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 942ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 892ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 929ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 730ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 683ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 824ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 762ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 935ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 849ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821ms/step\n",
      "  Model Name  Mean LOPO Accuracy  Mean K-Fold Accuracy\n",
      "0       LSTM            0.738095                   0.7\n"
     ]
    }
   ],
   "source": [
    "# # For all pairs\n",
    "# summary_df_lstm = evaluate_lstm_lofo_kfold(\n",
    "#     fish_sequences_padded=fish_sequences_padded,\n",
    "#     fish_labels_encoded=fish_labels_encoded,\n",
    "#     fish_nums=fish_nums,\n",
    "#     lopo_pairs=lopo_pairs,\n",
    "#     random_lopo_pairs=random_lopo_pairs,\n",
    "#     input_shape=input_shape,\n",
    "#     use_all_pairs=True \n",
    "# )\n",
    "# print(\"\\nLSTM Performance Summary:\")\n",
    "# print(summary_df_lstm.round(3).T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sta2453",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
